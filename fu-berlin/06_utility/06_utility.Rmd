---
title: "Advanced Survey Statistics: Disclosure Control"
subtitle: "Part 6: Utility"
author: "Matthias Templ"
date: FU-Berlin, 2019
output:
  beamer_presentation:
    includes:
      in_header: ../header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
theme_set(theme_grey() + theme(text=element_text(size = 20)))
```

## Measuring the data utility

complementary approaches to assess information loss: 

- distances between the original data and perturbed data 
- comparing statistics computed on the original and perturbed data. 
    * gerneral purpose statistics
    * data-specific measures
    
  
## Comparing missing values



 Let $\mathbf{R}^{(X)}$ and $\mathbf{R}^{(Y)}$ be indicator matrices of the same size as $\mathbf{X}$ (original data) and $\mathbf{Y}$ (anonymized data of the same size) with $n$ observations and $p$ variables. A cell/element of $\mathbf{R}^{(X)}$  is 1 when $\mathbf{X}$  has a missing value on that position, otherwise 0 (same for $\mathbf{R}^{(Y)}$).
 

\begin{equation*}
\tilde{r}_{ij} = \begin{cases}
0 & \text{if}\ r_{ij}^{(X)} = r_{ik}^{(Y)} = 0\quad , \\
0 & \text{if}\ r_{ij}^{(X)} = 1 \ \wedge \ r_{ij}^{(Y)} = 1\quad , \\
1 & \text{if}\ r_{ij}^{(X)} = 0 \ \wedge \ r_{ij}^{(Y)} = 1\quad , \\
0 & \text{if}\ r_{ij}^{(X)} = 1 \ \wedge \ r_{ij}^{(Y)} = 0\quad . \\
\end{cases}
\end{equation*}

<!-- Hereby, it is assumed that an original data set $X$ always has more or equal information as the anonymized -->
<!-- data set $Y$.  -->
<!-- Thus, a possible imputed value in $Y$ will not effect the following measure. -->

## Comparing missing values

Number of additional missings per variable caused by anonymizing the data using the indicator matrix $\mathbf{R}$ with $n$ observations and $p$ variables,

\begin{equation*}
m_j = \sum_i^n \tilde{r}_{ij} \quad , \  j \in \{1, \ldots, p\} \quad . 
\end{equation*}

<!-- This can also be seen relatively by dividing by the number of observations, or as percentages of new missing values in each variable. -->

Relative measure:

\begin{equation*}
mp_j = 100 \cdot \frac{m_j}{n}  \quad . 
\end{equation*}

The higher $m_j$ (or $mp_j$) the higher the information loss.

## Comparing missing values in R


```{r, echo=TRUE, message = FALSE, warning=FALSE}
library("laeken"); library("sdcMicro")
data("eusilc")
sdc <- createSdcObj(eusilc, 
          keyVars = c("db040", "hsize", "pb220a", 
                      "rb090"),
          weightVar = "rb050", hhId = "db030")
sdc <- kAnon(sdc) # local suppression produces additional NA's
print(sdc, "ls")
```

## Comparing contigency tables

Contingency table 
$\mathbf{T}^{(\mathbf{X})}$ calculated from categorical variables of the original data $\mathbf{X}$ and the contingency table $\mathbf{T}^{(\mathbf{Y})}$ from the anonymized data $\mathbf{Y}$. 

<!-- More precisely, the normed sum of the absolute distances between the cells of the tables with $n_1$ rows and $n_2$ columns \citep[see also the description from][]{domingo09}, -->

\begin{equation} \label{UT}
UT = \frac{1}{n_1 n_2} \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} \left| T_{ij}^{(\mathbf{X})} - T_{ij}^{(\mathbf{Y})} \right| \quad . 
\end{equation}

The higher $UT$ the lower the data quality.

## Comparing contigency tables


Relative change in each cell (in percentages).

\begin{equation} \label{UT}
UT2 = 100 \cdot \frac{1}{n_1 n_2} \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} \left| \frac{T_{ij}^{(\mathbf{X})}  - T_{ij}^{(\mathbf{Y})}}{T_{ij}^{(\mathbf{X})}} \right| \quad . 
\end{equation}

In the following code, a contingency table of $rb090 \times db040$ (gender $\times$ federal state) is computed for the anonymized data and original data.
First, an object of class \textit{sdcMicroObj} is created, and then we apply PRAM on federal state. 

Let's start from the beginning. First we create the \texttt{sdcMicro} object, then we apply PRAM and compare the original and table considering the prammed variable \texttt{db040} (region).

## Comparing contigency tables in R

```{r, echo=TRUE}
X <- Y <- eusilc
sdc <- createSdcObj(X, 
          keyVars = c("db040", "hsize", "pb220a", 
                      "rb090", "pl030", "age"),
          numVars = "eqIncome",
          pramVars = "db040",
          weightVar = "rb050", hhId = "db030")
sdc <- pram(sdc)
Y <- extractManipData(sdc)
```

## Comparing contigency tables in R

We now compare the tables according to Equation~(\ref{UT}).

```{r, echo=TRUE}
ct <- c("rb090", "db040")
Tx <- table(X[, ct])
Ty <- table(Y[, ct])
Tx
```

## Comparing contigency tables in R

```{r, echo=TRUE}
n1 <- nrow(Ty); n2 <- ncol(Ty)
## UT
sum(abs(Tx - Ty)) / (n1 * n2)
## UT2
sum(abs(Tx - Ty)/Tx) / (n1 * n2) * 100
```

$\rightarrow$ mean difference in the cell values of the tables is approximately $1.127\%$. 


## Comparing contigency tables, visually

```{r, echo=TRUE, eval = FALSE, fig.height=4, fig.width=4, message=FALSE, warning=FALSE, out.width="5cm"}
require(vcd)
ct <- c("rb090", "pb220a", "hsize")
library(simPop)
Tx <- tableWt(X[, ct], X$rb050)
Ty <- tableWt(Y[, ct], X$rb050)
par(mfrow=c(1,2))
mosaic(Tx); mosaic(Ty)
```



## Comparing contigency tables, visually

\begin{center}
\includegraphics[width=0.48\textwidth]{/Users/teml/workspace/sdc-springer/book/figures/mosaic2-1}
\includegraphics[width=0.48\textwidth]{/Users/teml/workspace/sdc-springer/book/figures/mosaic2-2}

Mosaic plot of gender (\texttt{rb090}) $\times$ citizenship (\texttt{pb220a}) $\times$ household size (\texttt{hsize}) showing the original sample frequencies (left plot) and the sample frequencies from the perturbed data (right plot).
\end{center}

## Comparing continuous key variables


- IL1s can be interpreted as the scaled distances between original and perturbed values. Again let $\mathbf{X}=\{ x_{ij}\}$ be the original data set, $\mathbf{Y}=\{ y_{ij}\}$ is a perturbed version of  $\mathbf{X}$. Both data sets consist of $n$ observations and $p$ variables each. The measure of information loss is defined by  
\begin{equation*}
IL1 = \frac{1}{pn} \sum\limits_{j=1}^p \sum\limits_{i=1}^n
\frac{ | x_{ij} - y_{ij} | }{ \sqrt{2} S_j} \quad ,
\end{equation*}
where $S_j$ is the standard deviation of the $j$-th variable in the original data set.

## Comparing continuous key variables

- \textbf{prediction quality} measures the differences between estimates obtained from fitting a pre-specified regression model on the original data and the perturbed data: 
\begin{equation*}
|(\bar{\hat{y}}_w^o-\bar{\hat{y}}_w^m)/\bar{\hat{y}}_w^o| \quad ,
\end{equation*} 
with
$\bar{\hat{y}}_w$ being fitted values from a pre-specified model obtained from
the original (index $o$) and the modified data (index $m$). Index $w$ indicates that
the survey weights should be considered when fitting the model.    


```{r, echo = TRUE}
sdc <- microaggregation(sdc)
get.sdcMicroObj(sdc, "utility")
```

## Mixed-scaled

$\rightarrow$ book

## Entropy

The alternative option is to use an entropy function. 
Given $c_1, c_2, \ldots, c_k$ categories of a variable $\mathbf{X_j}$, the 
entropy $E_{c_j}$ is defined as
\begin{equation}
E_{c_j} = - \frac{1}{n} \sum_{c_j \in \mathbf{X_j}} f_{c_j} log \left( \frac{f_{c_j}}{n} \right) \quad ,
\end{equation}
where $f_{c_j}$ is the frequency of category $c_j$ of variable $\mathbf{X}_j$ and $n$ the total number of observations. 

- Useful when recoding is done in an (semi-)automatized manner
- Useful when the choice of suppression variables is done in an automized manner
<!-- This could also be used to suppress variables, e.g. the variable with the largest value of -->
<!-- the entropy function is chosen for further global recoding (\textit{age} in our case, see below). -->

```{r, echo=TRUE}
## entropy of key variables on original data X
entropy <- function(fk, n){
  (-1) * 1 / n  * sum(fk * log(fk / n))
}
```

## Entropy

```{r, echo=TRUE}
## for hsize
n <- nrow(eusilc)
fk <- as.numeric(table(eusilc$hsize))
entropy(fk, n)
## for age
entropy(as.numeric(table(eusilc$age)), n)
## for pb220a
entropy(as.numeric(table(eusilc$pb220a)), n)
```

## Propensity Scores

- Rowbind $\mathbf{X}$ ($n$ observations) and $\mathbf{Y}$ ($m$ observations)
- Create indicator response variable that expresses memberships of observations to $\mathbf{X}$ and $\mathbf{Y}$
- logistic regression using the indicator variable as response
- predict prob. $p_i \quad , i = 1,.., n + m$ of the indicator variable
- Look at the differences 
\begin{equation*}
UP = \frac{1}{n+m} \sum_{i=1}^{n+m} (p_i - c)^2 \quad ,
\end{equation*}
where $p_i$ is the estimated probabilities being in group 1 (original data) or group 2 (perturbed data). $c$ is usually determined as 0.5. 
    * If $UP$ is close to zero, the data utility is high. 
    * worst case: $UP \sim 1/4$, the two data sets are completely distinguishable


## Propensity Scores in R

```{r, echo = TRUE}
Z <- rbind(eusilc, extractManipData(sdc))
Z$index <- rep(0:1, each=nrow(eusilc))
form <- as.formula("index ~ db040 + hsize + pb220a + 
                   rb090 + pl030 + eqIncome")
res <- glm(form, data=Z, family = binomial())
1 / nrow(Z) * sum((predict(res, type="response") - 0.5)^2)
```

$\rightarrow$ data utility is high. 

## Data-specific utility measures

If you release the data, what users of the data will analyse?
Determine the most important variables of the micro data set and take the most important indicators into account. Steps:

1. selection of a set of (benchmarking) indicators;
2. estimation of all benchmarking indicators based on the original micro data;
3. estimation of the benchmarking indicators based on the protected micro data set;
4. comparison of statistical properties such as point estimates,
  variances or overlaps in confidence intervals for each benchmarking indicator, regression coefficients, ...; 
5. assessment of the data utility of the protected micro
  data set 
  
## Example EU-SILC

Important indicators such as the Gini coefficient, the at-risk-at-poverty rate, ...

Given a vector $x_1, \ldots , x_n$ with sample weights $w_1, \ldots , w_n$ the Gini coefficient can be estimated by 
\begin{align*}
	\widehat{Gini} ~=~\frac{2\sum_{i=1}^n\left(w_i x_i \sum_{j=1}^i w_i\right)-\sum_{i=1}^n w_i^2 x_i}{\left(\sum_{i=1}^n w_i\right)\sum_{i=1}^n w_i x_i}-1 \quad .
\end{align*}
The Gini coefficient takes on values between 0 and 1. A value of 0 stand for perfect equality

## Example EU-SILC

Point estimates, relative difference in percent:

```{r, echo=TRUE}
sdc@additionalResults$gini <- gini(inc = "eqIncome",
       weigths = "rb050",
       breakdown = "db040", 
       data = extractManipData(sdc))$valueByStratum$value
```


```{r, echo=TRUE}
res <- gini(inc = "eqIncome",
            weigths = "rb050",
            breakdown = "db040", 
            data = eusilc)$valueByStratum$value
100*abs((res - sdc@additionalResults$gini)/res)
```

## Example EU-SILC

variance estimates

```{r, echo = TRUE}
res <- gini(inc = "eqIncome",
            weigths = "rb050",
            breakdown = "db040", # region
            data = eusilc)
resVar <- variance("eqIncome", weights = "rb050", 
            design = "db040", breakdown = "db040",
            data = eusilc, indicator = res, R = 50,
            X = calibVars(eusilc$db040), seed = 123)
res <- resVar$valueByStratum$value
resVar <- resVar$varByStratum$var
```

## Example EU-SILC

variance estimates

```{r, echo = TRUE}
eusilcA <- extractManipData(sdc)
resA <- gini(inc = "eqIncome",
           weigths = "rb050",
           breakdown = "db040", 
           data = eusilcA)
resVarA <- variance("eqIncome", weights = "rb050", 
            design = "db040", breakdown = "db040",
            data = eusilcA, indicator = resA, R = 50,
            X = calibVars(eusilc$db040), seed = 123)
resA <- resVarA$valueByStratum$value
resVarA <- resVarA$varByStratum$var
```

## Example EU-SILC

variance estimates: relative differences in percent

```{r, echo = TRUE}
100*abs((res - resA) / res)
100*abs((resVar - resVarA) / resVar)
```

Some kind of MSE:s

```{r, echo = TRUE}
(res - resA)^2 + abs(resVar - resVarA) # MSE
```

Alternative: Overlap of confidence intervals

## Conclusions

- \textbf{general purpose measures} such as IL1s, differences in means, outcome of multivariate statistical methods, propensity scores, distances, etc. are useful for giving a quick answer about the utility of the anonymized data set. 

- \textbf{data- and context-specific utility measures} such as useful regression models, the most interesting indicators, ...
    * gives more trustful indication on utility as general purpose measures
    * needs a lot of time to get sure about the user needs 




